[server]
host = "0.0.0.0"
grpc_port = 50051

[backends.llm.llama_cpp]
enabled = true
backend = "llama.cpp"
model = "models/llama-3-8b.Q4_K_M.gguf"
